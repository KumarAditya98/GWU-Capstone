%% Journal article
@misc{agrawal2016vqa,
      title={VQA: Visual Question Answering}, 
      author={Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},
      year={2016},
      eprint={1505.00468},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%% Journal article with DOI
@article{DBLP:journals/corr/ZhangGSBP15,
  author       = {Peng Zhang and
                  Yash Goyal and
                  Douglas Summers{-}Stay and
                  Dhruv Batra and
                  Devi Parikh},
  title        = {Yin and Yang: Balancing and Answering Binary Visual Questions},
  journal      = {CoRR},
  volume       = {abs/1511.05099},
  year         = {2015},
  url          = {http://arxiv.org/abs/1511.05099},
  eprinttype    = {arXiv},
  eprint       = {1511.05099},
  timestamp    = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ZhangGSBP15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%% Journal article
@misc{zhu2016visual7w,
      title={Visual7W: Grounded Question Answering in Images}, 
      author={Yuke Zhu and Oliver Groth and Michael Bernstein and Li Fei-Fei},
      year={2016},
      eprint={1511.03416},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%% book, authored
@misc{yu2015visual,
      title={Visual Madlibs: Fill in the blank Image Generation and Question Answering}, 
      author={Licheng Yu and Eunbyung Park and Alexander C. Berg and Tamara L. Berg},
      year={2015},
      eprint={1506.00278},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%% Item 8. Book, chapter
@misc{gao2015talking,
      title={Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering}, 
      author={Haoyuan Gao and Junhua Mao and Jie Zhou and Zhiheng Huang and Lei Wang and Wei Xu},
      year={2015},
      eprint={1505.05612},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ma2015learning,
      title={Learning to Answer Questions From Image Using Convolutional Neural Network}, 
      author={Lin Ma and Zhengdong Lu and Hang Li},
      year={2015},
      eprint={1506.00333},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%% Book, edited
@misc{malinowski2015ask,
      title={Ask Your Neurons: A Neural-based Approach to Answering Questions about Images}, 
      author={Mateusz Malinowski and Marcus Rohrbach and Mario Fritz},
      year={2015},
      eprint={1505.01121},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%% Chapter in a book in a series with volume titles
@inproceedings{andreas-etal-2016-learning,
    title = "Learning to Compose Neural Networks for Question Answering",
    author = "Andreas, Jacob  and
      Rohrbach, Marcus  and
      Darrell, Trevor  and
      Klein, Dan",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1181",
    doi = "10.18653/v1/N16-1181",
    pages = "1545--1554",
}

%% Paper presented at a conference
@misc{chen2016abccnn,
      title={ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering}, 
      author={Kan Chen and Jiang Wang and Liang-Chieh Chen and Haoyuan Gao and Wei Xu and Ram Nevatia},
      year={2016},
      eprint={1511.05960},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%% Data citation example
@misc{xu2016show,
      title={Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}, 
      author={Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron Courville and Ruslan Salakhutdinov and Richard Zemel and Yoshua Bengio},
      year={2016},
      eprint={1502.03044},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      eprint={2201.12086},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{li2021align,
      title={Align before Fuse: Vision and Language Representation Learning with Momentum Distillation}, 
      author={Junnan Li and Ramprasaath R. Selvaraju and Akhilesh Deepak Gotmare and Shafiq Joty and Caiming Xiong and Steven Hoi},
      year={2021},
      eprint={2107.07651},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2022simvlm,
      title={SimVLM: Simple Visual Language Model Pretraining with Weak Supervision}, 
      author={Zirui Wang and Jiahui Yu and Adams Wei Yu and Zihang Dai and Yulia Tsvetkov and Yuan Cao},
      year={2022},
      eprint={2108.10904},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{li2022blipblog,
author = {Li, Junnan},
title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
howpublished = {\url{https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/}},
year = {2022},
}

@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@misc{touvron2021training,
      title={Training data-efficient image transformers & distillation through attention}, 
      author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou},
      year={2021},
      eprint={2012.12877},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{krishna2016visual,
      title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations}, 
      author={Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li-Jia Li and David A. Shamma and Michael S. Bernstein and Fei-Fei Li},
      year={2016},
      eprint={1602.07332},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{changpinyo2021conceptual,
      title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts}, 
      author={Soravit Changpinyo and Piyush Sharma and Nan Ding and Radu Soricut},
      year={2021},
      eprint={2102.08981},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{NIPS2011_5dd9db5e,
 author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 volume = {24},
 year = {2011}
}

@misc{sahu2022vqa,
author = {Sahu, Tushar},
title = {Visual question answering with multimodal transformers},
howpublished = {\url{https://medium.com/data-science-at-microsoft/visual-question-answering-with-multimodal-transformers-d4f57950c867}},
year = {2022}
}

@article{WU201721,
title = {Visual question answering: A survey of methods and datasets},
journal = {Computer Vision and Image Understanding},
volume = {163},
pages = {21-40},
year = {2017},
note = {Language in Vision},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1077314217300772},
author = {Qi Wu and Damien Teney and Peng Wang and Chunhua Shen and Anthony Dick and Anton {van den Hengel}},
keywords = {Visual question answering, Natural language processing, Knowledge bases, Recurrent neural networks},
abstract = {Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{10.3115/1073083.1073135,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}
@Inproceedings{ImageCLEFVQA-Med2019,
    author = {Asma {Ben Abacha} and Sadid A. Hasan and Vivek V. Datla and Joey Liu and Dina Demner-Fushman and Henning M\"uller},
    
    title = {VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019},

    url = {https://ceur-ws.org/Vol-2380/paper\_272.pdf},
    
    booktitle = {Working Notes of {CLEF} 2019},
    
    series = {{CEUR} Workshop Proceedings},

    volume       = {2380},
    
    year = {2019},
    
    publisher    = {CEUR-WS.org}, 
    
    month = {September 9-12},
    
    address = {Lugano, Switzerland}
    }

@misc{vqa_rad,
  title={Visual Question Answering in Radiology (VQA-RAD)},
  url={osf.io/89kps},
  DOI={10.17605/OSF.IO/89KPS},
  publisher={OSF},
  author={Lau, Jason J and Gayen, Soumya and Demner, Dina and Ben Abacha, Asma},
  year={2019},
  month={Feb}
}
%%============================================================================%%
%% while using chicago reference style, both abbreviated and expanded form of %%
%% author name format is acceptable. Refer below example for expanded form    %%
%%============================================================================%%

%%  author		= "{Cameron, Deborah}", - single author
%%  author		= "{Saito, Yukio} and {Hyuga, Hiroyuki}", - double author 

%%======================================%%
%% Example for author names with suffix %%
%%======================================%%

%%  author		= "{Price, R. A. Jr} and {Curry, N. {III}} and McCann, K. E. and 
%%					Fielding, J. L. and {Abercrombie, E. Jr}",
